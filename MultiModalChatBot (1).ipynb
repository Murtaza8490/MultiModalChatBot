{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torch langchain langgraph transformers sentence-transformers faiss-cpu pillow diffusers"
      ],
      "metadata": {
        "id": "uLBL1CMdqVsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio langchain langgraph transformers sentence-transformers faiss-cpu pillow diffusers pyngrok torch\n",
        "!pip install torch langchain langgraph transformers sentence-transformers faiss-cpu pillow diffusers\n",
        "!pip install langchain-community\n",
        "!pip install bitsandbytes\n",
        "!pip install mistral_inference\n",
        "\n",
        "import os\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"\"\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from typing import List, Tuple\n",
        "from pyngrok import ngrok\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import torch\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langgraph.graph import END, MessageGraph\n",
        "from huggingface_hub import snapshot_download\n",
        "from pathlib import Path\n",
        "\n",
        "mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\n",
        "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "snapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n",
        "from transformers import pipeline\n",
        "\n",
        "class EnhancedChatBot:\n",
        "    # def __init__(self, ...):\n",
        "        # self.translator = pipeline(\"translation\",\n",
        "                                #  model=\"Helsinki-NLP/opus-mt-ur-en\",\n",
        "                                #  device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "    def translate_urdu(self, text: str) -> str:\n",
        "        return self.translator(text)[0]['translation_text']\n",
        "class EnhancedChatBot:\n",
        "    def __init__(self, language: str = \"english\"):\n",
        "        self.language = language\n",
        "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "        self.setup_models()\n",
        "        self.setup_rag()\n",
        "        self.setup_langgraph()\n",
        "\n",
        "    def setup_models(self):\n",
        "        # Text model with 4-bit quantization for Colab memory efficiency\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        self.text_llm = HuggingFacePipeline.from_model_id(\n",
        "            model_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "            task=\"text-generation\",\n",
        "            device_map=\"auto\",\n",
        "            model_kwargs={\n",
        "                \"quantization_config\": quant_config,\n",
        "                \"max_length\": 2048\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Image model with smaller footprint\n",
        "        from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        self.image_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "    def setup_rag(self):\n",
        "        from langchain_community.vectorstores import FAISS\n",
        "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "        self.vector_store = FAISS.from_texts(\n",
        "            [\"Sample AI information\", \"Example image context\"],\n",
        "            embedding=self.embeddings\n",
        "        )\n",
        "        self.retriever = self.vector_store.as_retriever()\n",
        "\n",
        "    def setup_langgraph(self):\n",
        "        from langgraph.graph import MessageGraph\n",
        "        self.workflow = MessageGraph()\n",
        "\n",
        "        def router(state: List):\n",
        "            last_msg = state[-1]\n",
        "            if isinstance(last_msg, HumanMessage):\n",
        "                return \"process_input\"\n",
        "            return END\n",
        "\n",
        "        self.workflow.add_node(\"process_input\", self.process_input)\n",
        "        self.workflow.set_entry_point(\"router\")\n",
        "        self.workflow.add_conditional_edges(\"router\", router)\n",
        "        self.workflow.add_edge(\"process_input\", END)\n",
        "\n",
        "    def process_input(self, messages: List) -> AIMessage:\n",
        "        user_input = messages[-1].content\n",
        "        image_path = getattr(messages[-1], \"image_path\", None)\n",
        "\n",
        "        # Process image if attached\n",
        "        image_context = \"\"\n",
        "        if image_path:\n",
        "            image_context = self.process_image(image_path)\n",
        "            os.remove(image_path)  # Cleanup temp file\n",
        "\n",
        "        # Generate response with memory\n",
        "        prompt = self.build_prompt(user_input, image_context)\n",
        "        response = self.text_llm.invoke(prompt)\n",
        "\n",
        "        # Update memory\n",
        "        self.memory.save_context({\"input\": user_input}, {\"output\": response})\n",
        "        return AIMessage(content=response)\n",
        "\n",
        "    def build_prompt(self, input_text: str, image_context: str = \"\") -> str:\n",
        "        history = self.memory.load_memory_variables({})[\"chat_history\"]\n",
        "        context = \"\\n\".join([d.page_content for d in self.retriever.get_relevant_documents(input_text)])\n",
        "\n",
        "        return f\"\"\"System: You are a helpful assistant. Use this context:\n",
        "        {context}\n",
        "        {image_context}\n",
        "\n",
        "        Conversation History:\n",
        "        {history}\n",
        "\n",
        "        User ({self.language}): {input_text}\n",
        "        Assistant:\"\"\"\n",
        "\n",
        "    def process_image(self, image_path: str) -> str:\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            inputs = self.processor(image, return_tensors=\"pt\").to(\"cuda\")\n",
        "            out = self.image_model.generate(**inputs)\n",
        "            return self.processor.decode(out[0], skip_special_tokens=True)\n",
        "        except Exception as e:\n",
        "            return f\"Image processing error: {str(e)}\"\n",
        "\n",
        "def gradio_interface():\n",
        "    bot = EnhancedChatBot()\n",
        "\n",
        "    def respond(message: str, image: Image.Image, history: List[Tuple], language: str):\n",
        "        bot.language = language\n",
        "\n",
        "        # Process input\n",
        "        if image:\n",
        "            image_path = \"/tmp/uploaded_image.jpg\"\n",
        "            image.save(image_path)\n",
        "            response = bot.workflow.invoke([HumanMessage(content=message, image_path=image_path)])\n",
        "        else:\n",
        "            response = bot.workflow.invoke([HumanMessage(content=message)])\n",
        "\n",
        "        history.append(((message, image), response.content))\n",
        "        return history, None, None\n",
        "\n",
        "    with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"# MultiModal ChatBot ðŸŒðŸ¤–\")\n",
        "\n",
        "        with gr.Row():\n",
        "            language = gr.Dropdown([\"english\", \"roman_urdu\"], label=\"Language\")\n",
        "            gr.Markdown(\"[[ Need help? ](https://example.com/docs)]\")\n",
        "\n",
        "        chatbot = gr.Chatbot(height=500)\n",
        "        msg = gr.Textbox(label=\"Your Message\")\n",
        "        image = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
        "        btn = gr.Button(\"Send\")\n",
        "\n",
        "        btn.click(respond, [msg, image, chatbot, language], [chatbot, msg, image])\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Colab deployment\n",
        "if __name__ == \"__main__\":\n",
        "    # Set ngrok token (create free account at https://ngrok.com)\n",
        "    NGROK_TOKEN = \"\"  # Replace with your token\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "    # Launch Gradio app\n",
        "    demo = gradio_interface()\n",
        "    public_url = ngrok.connect(7860).public_url\n",
        "    print(f\"Public URL: {public_url}\")\n",
        "    demo.launch(server_port=7861, share=True)"
      ],
      "metadata": {
        "id": "6AnD-vCuxhF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "class EnhancedChatBot:\n",
        "    def __init__(self, ...):\n",
        "        self.translator = pipeline(\"translation\",\n",
        "                                 model=\"Helsinki-NLP/opus-mt-ur-en\",\n",
        "                                 device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "    def translate_urdu(self, text: str) -> str:\n",
        "        return self.translator(text)[0]['translation_text']"
      ],
      "metadata": {
        "id": "z5R5dwktxjan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(self, ...):\n",
        "    if self.language == \"roman_urdu\":\n",
        "        input_text = self.translate_urdu(input_text)\n",
        "    # rest of the prompt building"
      ],
      "metadata": {
        "id": "zjBNcd5QxlWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key improvements for Colab deployment:\n",
        "\n",
        "Memory Optimization:\n",
        "\n",
        "4-bit model quantization for reduced memory usage\n",
        "\n",
        "Automatic image cleanup after processing\n",
        "\n",
        "Efficient GPU memory management\n",
        "\n",
        "Conversation Memory:\n",
        "\n",
        "Integrated LangChain memory buffer\n",
        "\n",
        "Full conversation history in prompts\n",
        "\n",
        "Session-aware responses\n",
        "\n",
        "Production-Ready Features:\n",
        "\n",
        "Ngrok tunneling for public access\n",
        "\n",
        "Error handling for image processing\n",
        "\n",
        "Clean UI with responsive components\n",
        "\n",
        "Language switching during conversation\n",
        "\n",
        "Colab-Specific Adjustments:\n",
        "\n",
        "Reduced model precision for better resource usage\n",
        "\n",
        "Temporary file handling\n",
        "\n",
        "Graceful error recovery\n",
        "\n",
        "Automatic dependency installation\n",
        "\n",
        "To use:\n",
        "\n",
        "Replace your_ngrok_token_here with your actual Ngrok token\n",
        "\n",
        "Run the entire notebook\n",
        "\n",
        "Wait for the public URL to appear (may take 1-2 minutes)\n",
        "\n",
        "Share the URL with users or test directly\n",
        "\n",
        "UI Features:\n",
        "\n",
        "Dual language support (toggle during conversation)\n",
        "\n",
        "Image drag-and-drop interface\n",
        "\n",
        "Conversation history scrollback\n",
        "\n",
        "Responsive mobile-friendly design\n",
        "\n",
        "Clear error messages\n",
        "\n",
        "Recommended Colab Setup:\n",
        "\n",
        "Use T4 GPU or higher\n",
        "\n",
        "Allocate at least 12GB RAM\n",
        "\n",
        "Enable high-RAM mode (Runtime > Change runtime type)\n"
      ],
      "metadata": {
        "id": "tHlLY3UZx8l9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-28VaR5yZyg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}